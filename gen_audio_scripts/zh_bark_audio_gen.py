import os
import pandas as pd
import time
import numpy as np
import soundfile as sf
from tqdm import tqdm
from transformers import AutoProcessor, BarkModel
import torch
import random
import sys

# ==== åƒæ•¸è¨­å®š ====
REF_TEXT_PATH = "F:/datasets/AISHELL-3/test/cleaned_aishell3_transcript.csv"
OUTPUT_DIR = "F:/datasets/generated_audio_bark/audio"
META_OUT_PATH = "F:/datasets/generated_audio_bark/meta_bark.csv"
SAMPLE_TARGET = 10000
BATCH_SIZE = 50
SLEEP_BETWEEN_BATCHES = 2
BARK_SPEAKERS = [f"v2/zh_speaker_{i}" for i in range(10)]

# ==== åˆå§‹åŒ– Bark ====
device = "cuda" if torch.cuda.is_available() else "cpu"
processor = AutoProcessor.from_pretrained("suno/bark")
model = BarkModel.from_pretrained("suno/bark").to(device)
sample_rate = model.generation_config.sample_rate

# ==== æº–å‚™è³‡æ–™ ====
os.makedirs(OUTPUT_DIR, exist_ok=True)

existing_files = {
    os.path.splitext(f)[0].replace("_bark", "")
    for f in os.listdir(OUTPUT_DIR)
    if f.endswith(".wav")
}
remaining = SAMPLE_TARGET - len(existing_files)
print(f"ğŸ¯ å·²æœ‰ {len(existing_files)} ç­†ï¼Œéœ€è¦è£œ {remaining} ç­†")

if remaining <= 0:
    print("âœ… æ•¸é‡é”æ¨™ï¼Œä¸éœ€è£œè³‡æ–™")
    sys.exit(0)

df = pd.read_csv(REF_TEXT_PATH)

# ==== ç¯©é¸é•·åº¦åœ¨ 3~10 ç§’ä¹‹é–“çš„åŸå§‹éŸ³æª” ====
def is_duration_ok(row):
    audio_path = os.path.join("F:/datasets/AISHELL-3/test/audio", row["filename"])
    try:
        with sf.SoundFile(audio_path) as f:
            dur = len(f) / f.samplerate
            return 3.0 <= dur <= 10.0
    except:
        return False

df["valid_duration"] = df.apply(is_duration_ok, axis=1)
df = df[df["valid_duration"]].reset_index(drop=True)

# æ’é™¤å·²ç”¢ç”Ÿéçš„èªéŸ³
df["base_name"] = df["filename"].str.replace(".wav", "")
df = df[~df["base_name"].isin(existing_files)].reset_index(drop=True)

if len(df) < remaining:
    print(f"âš ï¸ å¯ç”¨èªæ–™åªæœ‰ {len(df)} ç­†ï¼Œä¸è¶³ {remaining}ï¼Œå°‡å…¨éƒ¨ä½¿ç”¨")
    selected_df = df
else:
    selected_df = df.sample(n=remaining, random_state=42).reset_index(drop=True)

# ==== Bark èªéŸ³ç”Ÿæˆ ====
meta_rows = []
total = len(selected_df)
print(f"ğŸš€ é–‹å§‹è™•ç† {total} ç­†è³‡æ–™")

for start in range(0, total, BATCH_SIZE):
    end = min(start + BATCH_SIZE, total)
    batch_df = selected_df.iloc[start:end].reset_index(drop=True)
    print(f"\nğŸš€ è™•ç† Batch {start}-{end - 1} ...")

    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f"Batch {start}-{end - 1}"):
        transcript = row["transcript"]
        base_name = os.path.splitext(row["filename"])[0]
        output_filename = f"{base_name}_bark.wav"
        out_wav_path = os.path.join(OUTPUT_DIR, output_filename)
        speaker = random.choice(BARK_SPEAKERS)

        try:
            inputs = processor(transcript, voice_preset=speaker, return_tensors="pt")
            inputs['attention_mask'] = (inputs['input_ids'] != processor.tokenizer.pad_token_id).long()
            inputs = {k: v.to(device) for k, v in inputs.items()}

            audio_array = model.generate(**inputs).cpu().numpy().squeeze()
            audio_array = np.clip(audio_array, -1.0, 1.0)
            sf.write(out_wav_path, audio_array, samplerate=sample_rate)

            meta_rows.append({
                "id": len(meta_rows) + 1,
                "label": "spoof",
                "reference_audio_filename": row["filename"],
                "tts": "bark",
                "speaker": speaker,
                "output_filename": output_filename
            })

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±æ•—ï¼š{e}ï¼Œè·³é {row['filename']}")
            continue

    print(f"â¸ï¸ æ‰¹æ¬¡å®Œæˆï¼Œæš«åœ {SLEEP_BETWEEN_BATCHES} ç§’...\n")
    time.sleep(SLEEP_BETWEEN_BATCHES)

# ==== å„²å­˜ meta.csv ====
meta_df = pd.DataFrame(meta_rows)
meta_df.to_csv(META_OUT_PATH, index=False)
print(f"\nâœ… Bark å…¨éƒ¨å®Œæˆï¼å·²ç”Ÿæˆ {len(meta_df)} ç­†èªéŸ³ï¼Œmeta å·²å„²å­˜ã€‚")
